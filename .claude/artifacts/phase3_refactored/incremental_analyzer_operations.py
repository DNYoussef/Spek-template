"""
Extracted operations service from incremental_analyzer

Automatically generated by God Object Decomposer
"""

from typing import Dict, List, Optional, Any
import logging
from dataclasses import dataclass
from dataclasses import field
from collections import defaultdict
from pathlib import Path
import hashlib
from typing import Union
from typing import Any
import sys
from typing import Optional
import os
import time
import threading
import asyncio
from typing import List
from typing import Callable
import traceback
from typing import Dict
from typing import Tuple
from collections import deque
from typing import Set
from concurrent.futures import as_completed
from concurrent.futures import ThreadPoolExecutor


"""Validate file change record."""
def __post_init__(self):
        """Validate file change record."""
        assert self.file_path, "file_path cannot be empty"
        assert self.change_type in ['added', 'modified', 'deleted'], "Invalid change_type"
        assert self.timestamp > 0, "timestamp must be positive"

"""Add dependency relationship."""
def add_dependency(self, dependency_path: str) -> None:
        """Add dependency relationship."""
        self.dependencies.add(dependency_path)

"""Add dependent relationship."""
def add_dependent(self, dependent_path: str) -> None:
        """Add dependent relationship."""
        self.dependents.add(dependent_path)

"""Get all files that could be impacted by changes to this file."""
def get_impact_scope(self) -> Set[str]:
        """Get all files that could be impacted by changes to this file."""
        return self.dependents.copy()

"""Validate analysis task."""
def __post_init__(self):
        """Validate analysis task."""
        assert self.task_id, "task_id cannot be empty"
        assert self.file_path, "file_path cannot be empty"
        assert 1 <= self.priority <= 3, "priority must be 1-3"
        assert self.max_retries >= 0, "max_retries must be non-negative"

"""Initialize dependency graph analyzer."""
def __init__(self, max_depth: int = 10):
        """Initialize dependency graph analyzer."""
        self.max_depth = max_depth
        self.dependency_graph: Dict[str, DependencyNode] = {}
        self.graph_lock = threading.RLock()
        self.analysis_stats = {
            "nodes_analyzed": 0,
            "dependencies_discovered": 0,
            "impact_analyses": 0,
            "graph_updates": 0
        }
        
        logger.info(f"Initialized dependency graph analyzer with max depth: {max_depth}")

"""Add or update file node in dependency graph."""
def add_file_node(self, file_path: str, estimated_analysis_time: float = 100.0) -> DependencyNode:
        """Add or update file node in dependency graph."""
        with self.graph_lock:
            if file_path not in self.dependency_graph:
                self.dependency_graph[file_path] = DependencyNode(
                    file_path=file_path,
                    last_modified=time.time(),
                    estimated_analysis_time_ms=estimated_analysis_time
                )
                self.analysis_stats["nodes_analyzed"] += 1
            
            return self.dependency_graph[file_path]

"""Resolve module import to file path."""
def _resolve_import_path(self, module_name: str, base_dir: Path) -> Optional[Path]:
        """Resolve module import to file path."""
        # Handle relative imports
        if module_name.startswith('.'):
            parts = module_name.lstrip('.').split('.')
            if parts and parts[0]:
                potential_path = base_dir / '/'.join(parts) / '__init__.py'
                if potential_path.exists():
                    return potential_path
                potential_path = base_dir / f"{'/'.join(parts)}.py"
                if potential_path.exists():
                    return potential_path
        
        # Handle absolute imports - simplified resolution
        parts = module_name.split('.')
        
        # Search in common locations
        search_dirs = [base_dir, base_dir.parent, Path.cwd()]
        for search_dir in search_dirs:
            # Try as direct file
            potential_path = search_dir / f"{'/'.join(parts)}.py"
            if potential_path.exists():
                return potential_path
            
            # Try as package
            potential_path = search_dir / '/'.join(parts) / '__init__.py'
            if potential_path.exists():
                return potential_path
        
        return None

"""Analyze impact of file changes on dependent files.

NASA Rule 4: Function under 60 lines"""
def analyze_change_impact(self, changed_files: List[str]) -> Dict[str, Set[str]]:
        """
        Analyze impact of file changes on dependent files.
        
        NASA Rule 4: Function under 60 lines
        """
        impact_map = {}
        
        with self.graph_lock:
            for file_path in changed_files:
                impacted_files = set()
                
                if file_path in self.dependency_graph:
                    node = self.dependency_graph[file_path]
                    
                    # Direct dependents are immediately impacted
                    impacted_files.update(node.dependents)
                    
                    # Propagate impact through dependency chain
                    visited = {file_path}
                    to_visit = list(node.dependents)
                    depth = 0
                    
                    while to_visit and depth < self.max_depth:
                        next_level = []
                        
                        for dependent in to_visit:
                            if dependent not in visited:
                                visited.add(dependent)
                                impacted_files.add(dependent)
                                
                                if dependent in self.dependency_graph:
                                    next_level.extend(self.dependency_graph[dependent].dependents)
                        
                        to_visit = next_level
                        depth += 1
                
                impact_map[file_path] = impacted_files
                self.analysis_stats["impact_analyses"] += 1
        
        return impact_map

"""Get files ordered by analysis priority."""
def get_analysis_priority_order(self, files_to_analyze: List[str]) -> List[str]:
        """Get files ordered by analysis priority."""
        with self.graph_lock:
            file_priorities = []
            
            for file_path in files_to_analyze:
                if file_path in self.dependency_graph:
                    node = self.dependency_graph[file_path]
                    # Priority based on: dependency count + dependent count
                    priority_score = len(node.dependencies) + len(node.dependents)
                    file_priorities.append((priority_score, file_path))
                else:
                    # Unknown files get lowest priority
                    file_priorities.append((0, file_path))
            
            # Sort by priority (highest first)
            file_priorities.sort(key=lambda x: x[0], reverse=True)
            
            return [file_path for _, file_path in file_priorities]

"""Get dependency graph statistics."""
def get_dependency_stats(self) -> Dict[str, Any]:
        """Get dependency graph statistics."""
        with self.graph_lock:
            total_dependencies = sum(len(node.dependencies) for node in self.dependency_graph.values())
            total_dependents = sum(len(node.dependents) for node in self.dependency_graph.values())
            
            return {
                "total_nodes": len(self.dependency_graph),
                "total_dependencies": total_dependencies,
                "total_dependents": total_dependents,
                "average_dependencies_per_file": total_dependencies / max(len(self.dependency_graph), 1),
                "average_dependents_per_file": total_dependents / max(len(self.dependency_graph), 1),
                "analysis_stats": self.analysis_stats.copy()
            }

"""Initialize incremental analysis engine."""
def __init__(self, max_workers: Optional[int] = None):
        """Initialize incremental analysis engine."""
        self.max_workers = max_workers or min(8, max(2, threading.active_count()))
        self.thread_pool: Optional[ThreadPoolExecutor] = None
        
        # Core components
        self.dependency_analyzer = DependencyGraphAnalyzer()
        self.file_change_tracker = FileChangeTracker()
        
        # Analysis state
        self.analysis_active = False
        self.analysis_results: Dict[str, Any] = {}
        self.analysis_queue: deque = deque()
        self.analysis_stats = {
            "files_analyzed": 0,
            "incremental_analyses": 0,
            "full_analyses": 0,
            "analysis_time_saved_ms": 0.0,
            "cache_hits": 0
        }
        
        # Performance tracking
        self.analysis_times: Dict[str, float] = {}
        self.optimization_engine = None
        
        # Initialize optimization engine if available
        if ANALYZER_COMPONENTS_AVAILABLE:
            try:
                self.optimization_engine = get_global_optimization_engine()
            except Exception as e:
                logger.warning(f"Failed to initialize optimization engine: {e}")
        
        logger.info(f"Initialized incremental analysis engine with {self.max_workers} workers")

"""Determine analysis task type based on file change."""
def _determine_task_type(self, change: FileChangeRecord) -> str:
        """Determine analysis task type based on file change."""
        file_path = change.file_path
        
        if file_path.endswith('.py'):
            if change.change_type == 'added':
                return 'full_python_analysis'
            elif change.change_type == 'modified':
                return 'incremental_python_analysis'
            else:  # deleted
                return 'cleanup_analysis'
        
        elif file_path.endswith(('.yaml', '.yml', '.json')):
            return 'config_analysis'
        
        else:
            return 'generic_analysis'

"""Estimate analysis time based on file characteristics."""
def _estimate_analysis_time(self, file_path: str) -> float:
        """Estimate analysis time based on file characteristics."""
        try:
            file_size = Path(file_path).stat().st_size
            
            # Base time estimation (rough heuristics)
            if file_path.endswith('.py'):
                # Python files: ~1ms per KB + AST parsing overhead
                base_time = (file_size / 1024) * 1.5 + 50.0
            else:
                # Other files: ~0.5ms per KB
                base_time = (file_size / 1024) * 0.5 + 10.0
            
            # Historical adjustment if available
            if file_path in self.analysis_times:
                historical_time = self.analysis_times[file_path]
                # Weighted average: 70% historical, 30% estimated
                return historical_time * 0.7 + base_time * 0.3
            
            return base_time
            
        except Exception:
            # Fallback estimate
            return 100.0

"""Execute a single analysis task."""
def _execute_single_analysis_task(self, task: AnalysisTask) -> Dict[str, Any]:
        """Execute a single analysis task."""
        start_time = time.time()
        
        try:
            # Execute task based on type
            if task.task_type == 'full_python_analysis':
                result = self._analyze_python_file_full(task.file_path)
            elif task.task_type == 'incremental_python_analysis':
                result = self._analyze_python_file_incremental(task.file_path)
            elif task.task_type == 'cleanup_analysis':
                result = self._cleanup_deleted_file_analysis(task.file_path)
            elif task.task_type == 'config_analysis':
                result = self._analyze_config_file(task.file_path)
            else:
                result = self._analyze_generic_file(task.file_path)
            
            execution_time = (time.time() - start_time) * 1000
            
            return {
                "analysis_type": task.task_type,
                "file_path": task.file_path,
                "execution_time_ms": execution_time,
                "result_data": result,
                "success": True
            }
            
        except Exception as e:
            execution_time = (time.time() - start_time) * 1000
            logger.debug(f"Analysis task failed for {task.file_path}: {e}")
            
            return {
                "analysis_type": task.task_type,
                "file_path": task.file_path,
                "execution_time_ms": execution_time,
                "error": str(e),
                "success": False
            }

"""Perform full analysis on Python file."""
def _analyze_python_file_full(self, file_path: str) -> Dict[str, Any]:
        """Perform full analysis on Python file."""
        try:
            # Get AST from cache if available
            if ANALYZER_COMPONENTS_AVAILABLE and global_ast_cache:
                ast_tree = global_ast_cache.get_ast(file_path)
                cache_hit = True
            else:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                ast_tree = ast.parse(content, filename=file_path)
                cache_hit = False
            
            if cache_hit:
                self.analysis_stats["cache_hits"] += 1
            
            # Analyze AST structure
            analysis_result = {
                "ast_nodes": len(list(ast.walk(ast_tree))),
                "functions": len([n for n in ast.walk(ast_tree) if isinstance(n, ast.FunctionDef)]),
                "classes": len([n for n in ast.walk(ast_tree) if isinstance(n, ast.ClassDef)]),
                "imports": len([n for n in ast.walk(ast_tree) if isinstance(n, (ast.Import, ast.ImportFrom))]),
                "cache_hit": cache_hit
            }
            
            return analysis_result
            
        except Exception as e:
            return {"error": str(e), "analysis_type": "full_python"}

"""Perform incremental analysis on modified Python file."""
def _analyze_python_file_incremental(self, file_path: str) -> Dict[str, Any]:
        """Perform incremental analysis on modified Python file."""
        try:
            # For incremental analysis, we focus on changes only
            if ANALYZER_COMPONENTS_AVAILABLE:
                incremental_cache = get_global_incremental_cache()
                if incremental_cache:
                    # Track file change and get delta
                    delta = incremental_cache.track_file_change(file_path)
                    if delta:
                        return {
                            "analysis_type": "incremental",
                            "delta_info": delta,
                            "incremental_processing": True
                        }
            
            # Fallback to full analysis if incremental not available
            return self._analyze_python_file_full(file_path)
            
        except Exception as e:
            return {"error": str(e), "analysis_type": "incremental_python"}

"""Clean up analysis data for deleted file."""
def _cleanup_deleted_file_analysis(self, file_path: str) -> Dict[str, Any]:
        """Clean up analysis data for deleted file."""
        # Remove from dependency graph
        with self.dependency_analyzer.graph_lock:
            if file_path in self.dependency_analyzer.dependency_graph:
                node = self.dependency_analyzer.dependency_graph[file_path]
                
                # Remove from dependents
                for dependent in node.dependents:
                    if dependent in self.dependency_analyzer.dependency_graph:
                        self.dependency_analyzer.dependency_graph[dependent].dependencies.discard(file_path)
                
                # Remove from dependencies
                for dependency in node.dependencies:
                    if dependency in self.dependency_analyzer.dependency_graph:
                        self.dependency_analyzer.dependency_graph[dependency].dependents.discard(file_path)
                
                del self.dependency_analyzer.dependency_graph[file_path]
        
        # Clear from analysis times
        self.analysis_times.pop(file_path, None)
        
        return {
            "analysis_type": "cleanup",
            "file_removed": file_path,
            "cleanup_successful": True
        }

"""Perform generic file analysis."""
def _analyze_generic_file(self, file_path: str) -> Dict[str, Any]:
        """Perform generic file analysis."""
        try:
            file_stat = Path(file_path).stat()
            
            return {
                "file_size_bytes": file_stat.st_size,
                "last_modified": file_stat.st_mtime,
                "analysis_type": "generic"
            }
            
        except Exception as e:
            return {"error": str(e), "analysis_type": "generic"}

"""Get comprehensive performance statistics."""
def get_performance_stats(self) -> Dict[str, Any]:
        """Get comprehensive performance statistics."""
        total_analyses = (self.analysis_stats["incremental_analyses"] + 
                         self.analysis_stats["full_analyses"])
        
        incremental_ratio = (
            self.analysis_stats["incremental_analyses"] / max(total_analyses, 1)
        ) * 100
        
        return {
            "analysis_statistics": self.analysis_stats.copy(),
            "dependency_graph_stats": self.dependency_analyzer.get_dependency_stats(),
            "performance_metrics": {
                "incremental_analysis_ratio_percent": incremental_ratio,
                "average_analysis_time_ms": (
                    sum(self.analysis_times.values()) / max(len(self.analysis_times), 1)
                ),
                "total_time_saved_ms": self.analysis_stats["analysis_time_saved_ms"],
                "cache_hit_rate_percent": (
                    self.analysis_stats["cache_hits"] / max(self.analysis_stats["files_analyzed"], 1)
                ) * 100
            },
            "resource_utilization": {
                "max_workers": self.max_workers,
                "thread_pool_active": self.thread_pool is not None,
                "analysis_queue_size": len(self.analysis_queue)
            }
        }

"""Initialize file change tracker."""
def __init__(self, max_tracked_files: int = 10000):
        """Initialize file change tracker."""
        self.max_tracked_files = max_tracked_files
        self.file_hashes: Dict[str, str] = {}
        self.change_history: deque = deque(maxlen=1000)  # Last 1000 changes
        self.tracking_lock = threading.RLock()
        
        logger.info(f"Initialized file change tracker with max {max_tracked_files} files")

"""Calculate MD5 hash of file content."""
def _calculate_file_hash(self, file_path: str) -> Optional[str]:
        """Calculate MD5 hash of file content."""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
            return hashlib.md5(content, usedforsecurity=False).hexdigest()
        except Exception as e:
            logger.debug(f"Failed to calculate hash for {file_path}: {e}")
            return None

"""Get recent change history."""
def get_change_history(self, limit: int = 100) -> List[FileChangeRecord]:
        """Get recent change history."""
        with self.tracking_lock:
            return list(self.change_history)[-limit:]

"""Get file tracking statistics."""
def get_tracking_stats(self) -> Dict[str, Any]:
        """Get file tracking statistics."""
        with self.tracking_lock:
            return {
                "files_tracked": len(self.file_hashes),
                "changes_recorded": len(self.change_history),
                "max_tracked_files": self.max_tracked_files
            }

"""Get or create global incremental analysis engine."""
def get_global_incremental_engine() -> IncrementalAnalysisEngine:
    """Get or create global incremental analysis engine."""
    global _global_incremental_engine
    
    with _engine_lock:
        if _global_incremental_engine is None:
            _global_incremental_engine = IncrementalAnalysisEngine()
    
    return _global_incremental_engine

