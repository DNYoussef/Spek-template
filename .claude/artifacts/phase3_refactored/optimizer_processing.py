"""
Extracted processing service from optimizer

Automatically generated by God Object Decomposer
"""

from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from typing import Any
import sys
from typing import Optional
import os
from typing import Dict
from concurrent.futures import ThreadPoolExecutor


"""Parallel processing optimization for AST operations and analysis tasks.

NASA Rule 4: All methods under 60 lines
NASA Rule 7: Bounded resource usage"""
class ParallelProcessingOptimizer:
    """
    Parallel processing optimization for AST operations and analysis tasks.
    
    NASA Rule 4: All methods under 60 lines
    NASA Rule 7: Bounded resource usage
    """
    
    def __init__(self, max_workers: Optional[int] = None):
        """Initialize parallel processing optimizer."""
        self.max_workers = max_workers or min(8, psutil.cpu_count())
        self.thread_pool: Optional[ThreadPoolExecutor] = None
        self.optimization_stats = {
            "parallel_operations": 0,
            "sequential_operations": 0,
            "time_saved_ms": 0.0,
            "cpu_utilization_percent": 0.0
        }
        
        # Performance thresholds
        self.parallelization_threshold = 5  # Minimum items for parallel processing
        self.max_memory_per_worker_mb = 50.0  # Memory limit per worker
        
        logger.info(f"Initialized parallel optimizer with {self.max_workers} workers")
    
    def start_thread_pool(self) -> None:
        """Start thread pool for parallel operations."""
        if self.thread_pool is None:
            self.thread_pool = ThreadPoolExecutor(
                max_workers=self.max_workers,
                thread_name_prefix="PerformanceOptimizer"
            )
            logger.info(f"Started thread pool with {self.max_workers} workers")
    
    def stop_thread_pool(self) -> None:
        """Stop thread pool and cleanup resources."""
        if self.thread_pool:
            self.thread_pool.shutdown(wait=True)
            self.thread_pool = None
            logger.info("Stopped thread pool")
    
    async def optimize_parallel_processing(self,
                                        tasks: List[Callable],
                                        task_type: str = "analysis") -> OptimizationResult:
        """
        Optimize task execution using parallel processing.
        
        NASA Rule 4: Function under 60 lines
        NASA Rule 5: Input validation
        """
        assert isinstance(tasks, list), "tasks must be list"
        assert len(task_type) > 0, "task_type cannot be empty"
        
        if len(tasks) < self.parallelization_threshold:
            # Execute sequentially for small task counts
            return await self._execute_sequential(tasks, task_type)
        
        # Start thread pool if not already started
        if self.thread_pool is None:
            self.start_thread_pool()
        
        # Measure sequential baseline
        sequential_time = await self._measure_sequential_time(tasks[:3])  # Sample
        estimated_sequential_time = sequential_time * len(tasks) / 3
        
        # Execute parallel processing
        parallel_start = time.time()
        results = await self._execute_parallel(tasks)
        parallel_time = time.time() - parallel_start
        
        # Calculate improvement
        improvement_percent = ((estimated_sequential_time - parallel_time) / 
                              estimated_sequential_time) * 100
        
        # Update statistics
        self.optimization_stats["parallel_operations"] += len(tasks)
        self.optimization_stats["time_saved_ms"] += (estimated_sequential_time - parallel_time) * 1000
        
        success = improvement_percent > 20.0  # At least 20% improvement
        
        return OptimizationResult(
            target_name=f"{task_type}_parallel_processing",
            optimization_type="parallelization",
            baseline_time_ms=estimated_sequential_time * 1000,
            optimized_time_ms=parallel_time * 1000,
            improvement_percent=improvement_percent,
            memory_impact_mb=len(tasks) * 2.0,  # Estimated memory per task
            thread_impact=self.max_workers,
            success=success,
            details={
                "tasks_processed": len(tasks),
                "workers_used": self.max_workers,
                "successful_tasks": len([r for r in results if r is not None]),
                "failed_tasks": len([r for r in results if r is None])
            }
        )
    
    async def _execute_sequential(self, tasks: List[Callable], task_type: str) -> OptimizationResult:
        """Execute tasks sequentially for small task counts."""
        start_time = time.time()
        
        results = []
        for task in tasks:
            try:
                if asyncio.iscoroutinefunction(task):
                    result = await task()
                else:
                    result = task()
                results.append(result)
            except Exception as e:
                logger.debug(f"Sequential task failed: {e}")
                results.append(None)
        
        execution_time = time.time() - start_time
        
        self.optimization_stats["sequential_operations"] += len(tasks)
        
        return OptimizationResult(
            target_name=f"{task_type}_sequential_processing",
            optimization_type="sequential",
            baseline_time_ms=execution_time * 1000,
            optimized_time_ms=execution_time * 1000,
            improvement_percent=0.0,  # No improvement for sequential
            memory_impact_mb=len(tasks) * 1.0,
            thread_impact=1,
            success=True,
            details={
                "tasks_processed": len(tasks),
                "execution_mode": "sequential",
                "successful_tasks": len([r for r in results if r is not None])
            }
        )
    
    async def _measure_sequential_time(self, sample_tasks: List[Callable]) -> float:
        """Measure time for sequential execution of sample tasks."""
        start_time = time.time()
        
        for task in sample_tasks:
            try:
                if asyncio.iscoroutinefunction(task):
                    await task()
                else:
                    task()
            except Exception:
                pass  # Ignore errors for timing purposes
        
        return time.time() - start_time
    
    async def _execute_parallel(self, tasks: List[Callable]) -> List[Any]:
        """Execute tasks in parallel using thread pool."""
        if not self.thread_pool:
            raise RuntimeError("Thread pool not initialized")
        
        # Submit all tasks to thread pool
        futures = []
        for task in tasks:
            if asyncio.iscoroutinefunction(task):
                # Convert coroutine to regular function for thread pool
                future = self.thread_pool.submit(asyncio.run, task())
            else:
                future = self.thread_pool.submit(task)
            futures.append(future)
        
        # Collect results as they complete
        results = []
        for future in as_completed(futures, timeout=300):  # 5 minute timeout
            try:
                result = future.result(timeout=30)  # 30 second timeout per task
                results.append(result)
            except Exception as e:
                logger.debug(f"Parallel task failed: {e}")
                results.append(None)
        
        return results
    
    def get_parallel_processing_stats(self) -> Dict[str, Any]:
        """Get parallel processing statistics."""
        total_operations = (self.optimization_stats["parallel_operations"] + 
                          self.optimization_stats["sequential_operations"])
        
        parallelization_ratio = (
            self.optimization_stats["parallel_operations"] / max(total_operations, 1)
        ) * 100
        
        return {
            "max_workers": self.max_workers,
            "thread_pool_active": self.thread_pool is not None,
            "parallelization_ratio_percent": parallelization_ratio,
            "total_time_saved_ms": self.optimization_stats["time_saved_ms"],
            "operations_stats": self.optimization_stats.copy()
        }

"""Initialize parallel processing optimizer."""
def __init__(self, max_workers: Optional[int] = None):
        """Initialize parallel processing optimizer."""
        self.max_workers = max_workers or min(8, psutil.cpu_count())
        self.thread_pool: Optional[ThreadPoolExecutor] = None
        self.optimization_stats = {
            "parallel_operations": 0,
            "sequential_operations": 0,
            "time_saved_ms": 0.0,
            "cpu_utilization_percent": 0.0
        }
        
        # Performance thresholds
        self.parallelization_threshold = 5  # Minimum items for parallel processing
        self.max_memory_per_worker_mb = 50.0  # Memory limit per worker
        
        logger.info(f"Initialized parallel optimizer with {self.max_workers} workers")

"""Start thread pool for parallel operations."""
def start_thread_pool(self) -> None:
        """Start thread pool for parallel operations."""
        if self.thread_pool is None:
            self.thread_pool = ThreadPoolExecutor(
                max_workers=self.max_workers,
                thread_name_prefix="PerformanceOptimizer"
            )
            logger.info(f"Started thread pool with {self.max_workers} workers")

"""Stop thread pool and cleanup resources."""
def stop_thread_pool(self) -> None:
        """Stop thread pool and cleanup resources."""
        if self.thread_pool:
            self.thread_pool.shutdown(wait=True)
            self.thread_pool = None
            logger.info("Stopped thread pool")

"""Get parallel processing statistics."""
def get_parallel_processing_stats(self) -> Dict[str, Any]:
        """Get parallel processing statistics."""
        total_operations = (self.optimization_stats["parallel_operations"] + 
                          self.optimization_stats["sequential_operations"])
        
        parallelization_ratio = (
            self.optimization_stats["parallel_operations"] / max(total_operations, 1)
        ) * 100
        
        return {
            "max_workers": self.max_workers,
            "thread_pool_active": self.thread_pool is not None,
            "parallelization_ratio_percent": parallelization_ratio,
            "total_time_saved_ms": self.optimization_stats["time_saved_ms"],
            "operations_stats": self.optimization_stats.copy()
        }

