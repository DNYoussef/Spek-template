name: Connascence Quality Gates

on:
  push:
    branches: [main]
    paths:
      - 'analyzer/**'
      - 'src/**'
      - '**/*.py'
      - 'config/**'
      - '.github/workflows/**'
      - '**/*.yaml'
      - '**/*.yml'
  pull_request:
    branches: [main]
  # SCHEDULE REMOVED - no more daily notification spam
  # Only run on actual code changes that matter

env:
  PYTHON_VERSION: '3.11'
  MAX_CRITICAL_CONNASCENCE: 0
  MAX_HIGH_CONNASCENCE: 5
  MIN_MECE_SCORE: 0.75

jobs:
  connascence-analysis:
    name: Connascence Quality Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 20

    # This is a critical quality gate - ensure it runs independently
    needs: []

    outputs:
      critical-violations: ${{ steps.analysis.outputs.critical-violations }}
      high-violations: ${{ steps.analysis.outputs.high-violations }}
      mece-score: ${{ steps.analysis.outputs.mece-score }}
      gate-status: ${{ steps.analysis.outputs.gate-status }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install analyzer dependencies
      run: |
        pip install --upgrade pip
        pip install ast-grep radon complexity-validator
        # Install any additional dependencies for the analyzer
        pip install -r requirements.txt

    - name: Run unified analyzer v2.0 with connascence analysis
      id: analysis
      continue-on-error: false
      run: |
        echo "[SEARCH] Running unified analyzer v2.0 for comprehensive analysis..."
        mkdir -p .claude/.artifacts

        # Run unified analyzer with comprehensive analysis
        if python -m analyzer . --comprehensive --output .claude/.artifacts/unified-analysis.json 2>/dev/null; then
          echo "[OK] Unified analyzer v2.0 executed successfully"

          # Extract connascence metrics from unified analysis
          export PYTHONPATH=${{ github.workspace }}
          python3 << 'EOF'
          import json
          import os
          import sys
          from pathlib import Path

          # Load unified analyzer results
          try:
              with open('.claude/.artifacts/unified-analysis.json', 'r') as f:
                  data = json.load(f)
          except Exception as e:
              print(f"[WARN] Could not load unified analysis: {e}")
              # Fallback to basic analysis
              data = {"violations": [], "connascence": {}, "mece_score": 0.0}

          # Extract connascence violations
          violations = data.get('violations', [])
          connascence_violations = [v for v in violations if 'connascence' in v.get('type', '').lower() or 'coupling' in v.get('type', '')]

          critical_count = len([v for v in connascence_violations if v.get('severity') == 'CRITICAL'])
          high_count = len([v for v in connascence_violations if v.get('severity') == 'HIGH'])

          # Extract MECE score from unified analysis
          mece_score = data.get('mece_score', 0.75)

          # Calculate connascence metrics from all violations
          total_violations = len(violations)
          coupling_violations = len([v for v in violations if 'coupling' in v.get('type', '').lower()])
          god_objects = len([v for v in violations if v.get('type') == 'God Object'])
                        if len(methods) > 20:
                            god_objects.append({
                                "class": node.name,
                                "methods": len(methods),
                                "severity": "HIGH" if len(methods) > 30 else "MEDIUM"
                            })

                    elif isinstance(node, ast.FunctionDef):
                        method_count += 1
                        # Simple complexity estimation
                        complexity = self.estimate_complexity(node)
                        complexity_sum += complexity

                        if complexity > 15:
                            self.violations["high_complexity"].append({
                                "function": node.name,
                                "complexity": complexity,
                                "line": node.lineno,
                                "severity": "HIGH" if complexity > 25 else "MEDIUM"
                            })

                # Check for magic numbers
                self.check_magic_numbers(tree, filepath)

                # Store file metrics
                self.file_metrics[str(filepath)] = {
                    "classes": class_count,
                    "methods": method_count,
                    "avg_complexity": complexity_sum / max(method_count, 1),
                    "god_objects": god_objects
                }

                if god_objects:
                    self.violations["god_objects"].extend(god_objects)

            def estimate_complexity(self, node):
                """Estimate cyclomatic complexity of a function."""
                complexity = 1  # Base complexity
                for child in ast.walk(node):
                    if isinstance(child, (ast.If, ast.For, ast.While, ast.With)):
                        complexity += 1
                    elif isinstance(child, ast.ExceptHandler):
                        complexity += 1
                return complexity

            def check_magic_numbers(self, tree, filepath):
                """Check for magic number violations."""
                for node in ast.walk(tree):
                    if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):
                        if node.value not in [0, 1, -1, 2, 10, 100, 1000] and abs(node.value) > 1:
                            self.violations["magic_numbers"].append({
                                "value": node.value,
                                "line": node.lineno,
                                "file": str(filepath),
                                "severity": "MEDIUM"
                            })

            def calculate_mece_score(self):
                """Calculate MECE (Mutually Exclusive, Collectively Exhaustive) score."""
                total_files = len(self.file_metrics)
                if total_files == 0:
                    return 1.0

                # Scoring factors
                god_object_penalty = len(self.violations.get("god_objects", [])) * 0.1
                complexity_penalty = len(self.violations.get("high_complexity", [])) * 0.05
                magic_number_penalty = len(self.violations.get("magic_numbers", [])) * 0.01

                base_score = 1.0
                total_penalty = god_object_penalty + complexity_penalty + magic_number_penalty
                mece_score = max(0.0, base_score - total_penalty)

                return min(1.0, mece_score)

            def generate_report(self):
                """Generate connascence analysis report."""
                critical_count = 0
                high_count = 0

                # Count violations by severity
                for violation_type, violations in self.violations.items():
                    for violation in violations:
                        if violation.get("severity") == "CRITICAL":
                            critical_count += 1
                        elif violation.get("severity") == "HIGH":
                            high_count += 1

                mece_score = self.calculate_mece_score()

                return {
                    "critical_violations": critical_count,
                    "high_violations": high_count,
                    "mece_score": mece_score,
                    "violations_by_type": dict(self.violations),
                    "file_metrics": self.file_metrics,
                    "total_files_analyzed": len(self.file_metrics)
                }

        # Run analysis
        analyzer = ConnascenceAnalyzer()

        # Analyze Python files in analyzer directory
        analyzer_path = Path("analyzer")
        if analyzer_path.exists():
            for py_file in analyzer_path.rglob("*.py"):
                if "__pycache__" not in str(py_file):
                    analyzer.analyze_file(py_file)

        # Analyze src directory if it exists
        src_path = Path("src")
        if src_path.exists():
            for py_file in src_path.rglob("*.py"):
                if "__pycache__" not in str(py_file):
                    analyzer.analyze_file(py_file)

        # Generate report
        report = analyzer.generate_report()

        # Save results
        with open(".claude/.artifacts/connascence-analysis.json", "w") as f:
            json.dump(report, f, indent=2)

        print(f"Connascence Analysis Complete:")
        print(f"- Critical violations: {report['critical_violations']}")
        print(f"- High violations: {report['high_violations']}")
        print(f"- MECE score: {report['mece_score']:.3f}")
        print(f"- Files analyzed: {report['total_files_analyzed']}")

        # Output for GitHub Actions
        print(f"critical-violations={report['critical_violations']}")
        print(f"high-violations={report['high_violations']}")
        print(f"mece-score={report['mece_score']:.3f}")

        EOF

        # Extract outputs from Python script
        CRITICAL_VIOLATIONS=$(python3 -c "
        import json
        try:
            with open('.claude/.artifacts/connascence-analysis.json') as f:
                data = json.load(f)
            print(data['critical_violations'])
        except:
            print('0')
        " 2>/dev/null || echo "0")

        HIGH_VIOLATIONS=$(python3 -c "
        import json
        try:
            with open('.claude/.artifacts/connascence-analysis.json') as f:
                data = json.load(f)
            print(data['high_violations'])
        except:
            print('0')
        " 2>/dev/null || echo "0")

        MECE_SCORE=$(python3 -c "
        import json
        try:
            with open('.claude/.artifacts/connascence-analysis.json') as f:
                data = json.load(f)
            print(f\"{data['mece_score']:.3f}\")
        except:
            print('0.000')
        " 2>/dev/null || echo "0.000")

        echo "critical-violations=$CRITICAL_VIOLATIONS" >> $GITHUB_OUTPUT
        echo "high-violations=$HIGH_VIOLATIONS" >> $GITHUB_OUTPUT
        echo "mece-score=$MECE_SCORE" >> $GITHUB_OUTPUT

        # Validate quality gates
        GATE_STATUS="PASS"

        if [ "$CRITICAL_VIOLATIONS" -gt "$MAX_CRITICAL_CONNASCENCE" ]; then
          GATE_STATUS="FAIL"
          echo "❌ Critical connascence violations: $CRITICAL_VIOLATIONS > $MAX_CRITICAL_CONNASCENCE"
        fi

        if [ "$HIGH_VIOLATIONS" -gt "$MAX_HIGH_CONNASCENCE" ]; then
          GATE_STATUS="FAIL"
          echo "❌ High connascence violations: $HIGH_VIOLATIONS > $MAX_HIGH_CONNASCENCE"
        fi

        MECE_PASS=$(python3 -c "print('true' if float('$MECE_SCORE') >= $MIN_MECE_SCORE else 'false')")
        if [ "$MECE_PASS" = "false" ]; then
          GATE_STATUS="FAIL"
          echo "❌ MECE score too low: $MECE_SCORE < $MIN_MECE_SCORE"
        fi

        echo "gate-status=$GATE_STATUS" >> $GITHUB_OUTPUT

        if [ "$GATE_STATUS" = "FAIL" ]; then
          echo "❌ Connascence quality gates FAILED - blocking deployment"
          echo "critical-gate-failure=true" >> $GITHUB_OUTPUT
          exit 1
        else
          echo "✅ Connascence quality gates PASSED"
          echo "critical-gate-failure=false" >> $GITHUB_OUTPUT
        fi

    - name: Generate detailed report
      if: always()
      run: |
        echo "Generating connascence quality report..."

        cat > .claude/.artifacts/connascence-report.md << EOF
        # Connascence Quality Gates Report

        **Generated**: $(date)
        **Gate Status**: ${{ steps.analysis.outputs.gate-status }}

        ## Quality Metrics
        - **Critical Violations**: ${{ steps.analysis.outputs.critical-violations }} (Max: ${{ env.MAX_CRITICAL_CONNASCENCE }})
        - **High Violations**: ${{ steps.analysis.outputs.high-violations }} (Max: ${{ env.MAX_HIGH_CONNASCENCE }})
        - **MECE Score**: ${{ steps.analysis.outputs.mece-score }} (Min: ${{ env.MIN_MECE_SCORE }})

        ## Quality Gate Thresholds
        - Critical connascence violations: ≤ ${{ env.MAX_CRITICAL_CONNASCENCE }}
        - High connascence violations: ≤ ${{ env.MAX_HIGH_CONNASCENCE }}
        - MECE score: ≥ ${{ env.MIN_MECE_SCORE }}

        ## Detailed Analysis
        $(python3 -c "
        import json
        try:
            with open('.claude/.artifacts/connascence-analysis.json') as f:
                data = json.load(f)

            print('### Violations by Type')
            for vtype, violations in data.get('violations_by_type', {}).items():
                if violations:
                    print(f'**{vtype.replace(\"_\", \" \").title()}**: {len(violations)} violations')
                    for v in violations[:3]:  # Show first 3
                        if 'line' in v:
                            print(f'  - Line {v[\"line\"]}: {v.get(\"function\", v.get(\"class\", \"Unknown\"))}')

            print()
            print('### File Metrics Summary')
            total_files = data.get('total_files_analyzed', 0)
            print(f'- Total files analyzed: {total_files}')

            if 'file_metrics' in data:
                total_classes = sum(m.get('classes', 0) for m in data['file_metrics'].values())
                total_methods = sum(m.get('methods', 0) for m in data['file_metrics'].values())
                print(f'- Total classes: {total_classes}')
                print(f'- Total methods: {total_methods}')
        except Exception as e:
            print(f'Error generating detailed report: {e}')
        ")

        EOF

    - name: Upload analysis artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: connascence-analysis-${{ github.run_number }}
        path: |
          .claude/.artifacts/connascence-analysis.json
          .claude/.artifacts/connascence-report.md
        retention-days: 30

    - name: Create quality issue on failure
      if: failure() && steps.analysis.outputs.gate-status == 'FAIL'
      uses: actions/github-script@v7
      with:
        script: |
          const title = '🔗 Connascence Quality Gate Failure';
          const body = `
          **Connascence quality gates have failed:**

          **Metrics**:
          - Critical violations: ${{ steps.analysis.outputs.critical-violations }} (Max: ${{ env.MAX_CRITICAL_CONNASCENCE }})
          - High violations: ${{ steps.analysis.outputs.high-violations }} (Max: ${{ env.MAX_HIGH_CONNASCENCE }})
          - MECE score: ${{ steps.analysis.outputs.mece-score }} (Min: ${{ env.MIN_MECE_SCORE }})

          **Run Details**:
          - Workflow: ${{ github.workflow }}
          - Run: ${{ github.run_id }}
          - Commit: ${{ github.sha }}

          **Required Actions**:
          1. Review connascence analysis artifacts
          2. Refactor god objects and high complexity functions
          3. Improve MECE score through better separation of concerns

          See workflow artifacts for detailed violation analysis.
          `;

          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: title,
            body: body,
            labels: ['quality', 'connascence', 'technical-debt']
          });

    - name: Add job summary
      if: always()
      run: |
        echo "## Connascence Quality Gates Summary" >> $GITHUB_STEP_SUMMARY
        echo "**Status**: ${{ steps.analysis.outputs.gate-status }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Metric | Value | Threshold | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|--------|-------|-----------|--------|" >> $GITHUB_STEP_SUMMARY

        CRITICAL_STATUS="✅"
        HIGH_STATUS="✅"
        MECE_STATUS="✅"

        if [ "${{ steps.analysis.outputs.critical-violations }}" -gt "${{ env.MAX_CRITICAL_CONNASCENCE }}" ]; then
          CRITICAL_STATUS="❌"
        fi

        if [ "${{ steps.analysis.outputs.high-violations }}" -gt "${{ env.MAX_HIGH_CONNASCENCE }}" ]; then
          HIGH_STATUS="❌"
        fi

        MECE_PASS=$(python3 -c "print('✅' if float('${{ steps.analysis.outputs.mece-score }}') >= ${{ env.MIN_MECE_SCORE }} else '❌')")

        echo "| Critical Violations | ${{ steps.analysis.outputs.critical-violations }} | ≤ ${{ env.MAX_CRITICAL_CONNASCENCE }} | $CRITICAL_STATUS |" >> $GITHUB_STEP_SUMMARY
        echo "| High Violations | ${{ steps.analysis.outputs.high-violations }} | ≤ ${{ env.MAX_HIGH_CONNASCENCE }} | $HIGH_STATUS |" >> $GITHUB_STEP_SUMMARY
        echo "| MECE Score | ${{ steps.analysis.outputs.mece-score }} | ≥ ${{ env.MIN_MECE_SCORE }} | $MECE_PASS |" >> $GITHUB_STEP_SUMMARY