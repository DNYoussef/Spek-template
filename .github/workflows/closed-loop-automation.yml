name: Closed-Loop GitHub Automation with Real-Time Feedback
on:
  push:
    branches: [main, develop]
    paths:
      - 'src/**'
      - 'analyzer/**' 
      - 'scripts/**'
      - '.github/workflows/**'
  pull_request:
    branches: [main, develop]
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours for proactive monitoring
  workflow_dispatch:
    inputs:
      recovery_mode:
        description: 'Recovery operation mode'
        required: false
        default: 'automatic'
        type: choice
        options:
          - automatic
          - supervised
          - analysis_only
      failure_category:
        description: 'Target failure category for recovery'
        required: false
        type: choice
        options:
          - all
          - quality_gates
          - security
          - performance
          - dependencies

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '20'
  AUTOMATION_VERSION: 'v2.0.0'
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  RECOVERY_MODE: ${{ github.event.inputs.recovery_mode || 'automatic' }}

jobs:
  # Real-time CI/CD Event Detection and Processing
  event_detection_hub:
    runs-on: ubuntu-latest-2-core
    name: "CI/CD Event Detection & Categorization Hub"
    timeout-minutes: 10
    outputs:
      failure_detected: ${{ steps.detection.outputs.failure_detected }}
      failure_category: ${{ steps.detection.outputs.failure_category }}
      recovery_required: ${{ steps.detection.outputs.recovery_required }}
      automation_strategy: ${{ steps.detection.outputs.automation_strategy }}

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 2  # Need previous commit for comparison
        token: ${{ env.GITHUB_TOKEN }}

    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Detection Dependencies
      run: |
        echo "Installing GitHub API and automation dependencies..."
        pip install --upgrade pip
        pip install PyGithub requests python-dateutil pyyaml
        pip install ghapi httpx asyncio

    - name: Real-Time Failure Detection & Categorization
      id: detection
      run: |
        echo "::group::Real-Time CI/CD Failure Detection"
        echo "Analyzing recent workflow runs and commit status for intelligent failure detection..."
        
        python -c "
        import os
        import sys
        import json
        import requests
        from datetime import datetime, timedelta
        from pathlib import Path
        
        def detect_cicd_failures():
            print('[SEARCH] REAL-TIME CI/CD FAILURE DETECTION SYSTEM')
            print('=' * 50)
            
            # GitHub API setup
            github_token = os.environ.get('GITHUB_TOKEN')
            repo_owner = os.environ.get('GITHUB_REPOSITORY_OWNER', '')
            repo_name = os.environ.get('GITHUB_REPOSITORY', '').split('/')[-1]
            
            if not all([github_token, repo_owner, repo_name]):
                print('::warning::GitHub API credentials incomplete - using simulated detection')
                return simulate_failure_detection()
            
            headers = {
                'Authorization': f'token {github_token}',
                'Accept': 'application/vnd.github.v3+json'
            }
            
            base_url = f'https://api.github.com/repos/{repo_owner}/{repo_name}'
            
            # Analyze recent workflow runs
            try:
                workflows_url = f'{base_url}/actions/runs?per_page=20'
                response = requests.get(workflows_url, headers=headers)
                
                if response.status_code == 200:
                    workflow_data = response.json()
                    return analyze_workflow_failures(workflow_data)
                else:
                    print(f'::warning::GitHub API call failed: {response.status_code}')
                    return simulate_failure_detection()
                    
            except Exception as e:
                print(f'::warning::GitHub API error: {e}')
                return simulate_failure_detection()
        
        def analyze_workflow_failures(workflow_data):
            print('[CHART] Analyzing Recent Workflow Runs...')
            
            workflow_runs = workflow_data.get('workflow_runs', [])
            recent_runs = [run for run in workflow_runs[:10]]  # Last 10 runs
            
            failure_analysis = {
                'total_runs': len(recent_runs),
                'failed_runs': 0,
                'failure_categories': {},
                'failure_patterns': [],
                'recovery_recommendations': []
            }
            
            failure_detected = False
            primary_failure_category = 'none'
            
            for run in recent_runs:
                run_status = run.get('status', 'unknown')
                run_conclusion = run.get('conclusion', 'unknown')
                workflow_name = run.get('name', 'unknown')
                
                print(f'  [CLIPBOARD] {workflow_name}: {run_status}/{run_conclusion}')
                
                if run_conclusion in ['failure', 'timed_out', 'cancelled']:
                    failure_analysis['failed_runs'] += 1
                    failure_detected = True
                    
                    # Categorize failure based on workflow name and patterns
                    category = categorize_workflow_failure(workflow_name, run)
                    
                    if category not in failure_analysis['failure_categories']:
                        failure_analysis['failure_categories'][category] = 0
                    failure_analysis['failure_categories'][category] += 1
            
            # Determine primary failure category
            if failure_analysis['failure_categories']:
                primary_failure_category = max(failure_analysis['failure_categories'].keys(), 
                                             key=lambda x: failure_analysis['failure_categories'][x])
            
            # Calculate failure rate and recovery strategy
            failure_rate = (failure_analysis['failed_runs'] / max(failure_analysis['total_runs'], 1)) * 100
            
            recovery_required = failure_detected and failure_rate >= 20  # 20% failure threshold
            automation_strategy = determine_automation_strategy(failure_analysis, failure_rate)
            
            print(f'\\n[TARGET] FAILURE DETECTION RESULTS:')
            print(f'  - Failure detected: {failure_detected}')
            print(f'  - Primary category: {primary_failure_category}')
            print(f'  - Failure rate: {failure_rate:.1f}% ({failure_analysis[\"failed_runs\"]}/{failure_analysis[\"total_runs\"]})')
            print(f'  - Recovery required: {recovery_required}')
            print(f'  - Automation strategy: {automation_strategy}')
            
            # Set GitHub Actions outputs
            print(f'::set-output name=failure_detected::{failure_detected}')
            print(f'::set-output name=failure_category::{primary_failure_category}')
            print(f'::set-output name=recovery_required::{recovery_required}')
            print(f'::set-output name=automation_strategy::{automation_strategy}')
            
            # Save detection results for subsequent jobs
            detection_report = {
                'timestamp': datetime.now().isoformat(),
                'failure_detected': failure_detected,
                'failure_category': primary_failure_category,
                'recovery_required': recovery_required,
                'automation_strategy': automation_strategy,
                'failure_analysis': failure_analysis,
                'failure_rate': failure_rate
            }
            
            Path('.github/automation').mkdir(parents=True, exist_ok=True)
            with open('.github/automation/failure_detection.json', 'w') as f:
                json.dump(detection_report, f, indent=2)
            
            return detection_report
        
        def categorize_workflow_failure(workflow_name, run_data):
            \"\"\"Categorize failure based on workflow name and context\"\"\"
            workflow_lower = workflow_name.lower()
            
            if any(term in workflow_lower for term in ['quality', 'gate', 'analysis']):
                return 'quality_gates'
            elif any(term in workflow_lower for term in ['security', 'scan', 'vulnerability']):
                return 'security'
            elif any(term in workflow_lower for term in ['performance', 'benchmark', 'load']):
                return 'performance'
            elif any(term in workflow_lower for term in ['test', 'unit', 'integration']):
                return 'testing'
            elif any(term in workflow_lower for term in ['build', 'compile', 'deploy']):
                return 'build_deploy'
            elif any(term in workflow_lower for term in ['dependency', 'audit', 'update']):
                return 'dependencies'
            else:
                return 'general'
        
        def determine_automation_strategy(failure_analysis, failure_rate):
            \"\"\"Determine optimal automation strategy based on failure patterns\"\"\"
            recovery_mode = os.environ.get('RECOVERY_MODE', 'automatic')
            
            if recovery_mode == 'analysis_only':
                return 'analysis_only'
            elif failure_rate >= 50:
                return 'immediate_intervention'
            elif failure_rate >= 30:
                return 'supervised_recovery'
            elif failure_rate >= 10:
                return 'automatic_retry'
            else:
                return 'monitoring_only'
        
        def simulate_failure_detection():
            \"\"\"Fallback simulation for environments without GitHub API access\"\"\"
            print('[WARN] Using simulated failure detection (GitHub API unavailable)')
            
            # Check for common failure indicators in repository
            common_failure_indicators = [
                Path('.github/workflows').exists(),
                Path('package.json').exists(),
                Path('requirements.txt').exists()
            ]
            
            simulated_failure = any(common_failure_indicators)
            
            print(f'::set-output name=failure_detected::{simulated_failure}')
            print(f'::set-output name=failure_category::quality_gates')
            print(f'::set-output name=recovery_required::{simulated_failure}')
            print(f'::set-output name=automation_strategy::automatic_retry')
            
            return {
                'failure_detected': simulated_failure,
                'failure_category': 'quality_gates',
                'recovery_required': simulated_failure,
                'automation_strategy': 'automatic_retry'
            }
        
        # Execute failure detection
        detection_result = detect_cicd_failures()
        "
        
        echo "::endgroup::"

    - name: Upload Failure Detection Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: failure-detection-report-${{ github.run_number }}
        path: |
          .github/automation/failure_detection.json
        retention-days: 7

  # Intelligent Recovery Orchestration
  intelligent_recovery_orchestrator:
    needs: event_detection_hub
    if: needs.event_detection_hub.outputs.recovery_required == 'true'
    runs-on: ubuntu-latest-4-core
    name: "Intelligent Recovery & Auto-Fix Orchestrator"
    timeout-minutes: 45

    strategy:
      fail-fast: false
      matrix:
        recovery_stream:
          - name: "dependency_recovery"
            timeout: 15
            priority: "critical"
          - name: "quality_gate_recovery"
            timeout: 20
            priority: "high"
          - name: "security_issue_recovery"
            timeout: 15
            priority: "high"
          - name: "performance_recovery"
            timeout: 20
            priority: "medium"

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ env.GITHUB_TOKEN }}

    - name: Set up Recovery Environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Recovery Tools
      run: |
        echo "Installing comprehensive recovery and automation tools..."
        pip install --upgrade pip
        pip install PyGithub ghapi requests pyyaml
        pip install black isort autoflake safety bandit semgrep
        pip install pre-commit autopep8
        
        # Install Node.js tools for frontend recovery
        if [ -f package.json ]; then
          npm ci || npm install
        fi

    - name: Download Failure Detection Data
      uses: actions/download-artifact@v4
      with:
        name: failure-detection-report-${{ github.run_number }}
        path: .github/automation/
      continue-on-error: true

    - name: Execute Intelligent Recovery Stream
      timeout-minutes: ${{ matrix.recovery_stream.timeout }}
      run: |
        echo "::group::Intelligent Recovery: ${{ matrix.recovery_stream.name }}"
        echo "Executing targeted recovery for detected failures..."
        
        python -c "
        import os
        import sys
        import json
        import subprocess
        import time
        from pathlib import Path
        from datetime import datetime
        
        def execute_recovery_stream():
            print('[WRENCH] INTELLIGENT RECOVERY ORCHESTRATOR')
            print(f'Recovery Stream: ${{ matrix.recovery_stream.name }}')
            print(f'Priority: ${{ matrix.recovery_stream.priority }}')
            print('=' * 50)
            
            # Load failure detection data
            failure_data = load_failure_detection_data()
            failure_category = failure_data.get('failure_category', 'general')
            automation_strategy = failure_data.get('automation_strategy', 'monitoring_only')
            
            print(f'[TARGET] Target failure category: {failure_category}')
            print(f'ðŸ¤– Automation strategy: {automation_strategy}')
            
            recovery_stream = '${{ matrix.recovery_stream.name }}'
            recovery_results = []
            
            # Route to appropriate recovery handler
            if recovery_stream == 'dependency_recovery':
                recovery_results = handle_dependency_recovery(failure_category)
            elif recovery_stream == 'quality_gate_recovery':
                recovery_results = handle_quality_gate_recovery(failure_category)
            elif recovery_stream == 'security_issue_recovery':
                recovery_results = handle_security_recovery(failure_category)
            elif recovery_stream == 'performance_recovery':
                recovery_results = handle_performance_recovery(failure_category)
            
            # Save recovery results
            save_recovery_results(recovery_stream, recovery_results)
            
            return recovery_results
        
        def load_failure_detection_data():
            \"\"\"Load failure detection data from previous job\"\"\"
            failure_file = Path('.github/automation/failure_detection.json')
            
            if failure_file.exists():
                try:
                    with open(failure_file, 'r') as f:
                        return json.load(f)
                except Exception as e:
                    print(f'Warning: Could not load failure data: {e}')
            
            # Return default failure data
            return {
                'failure_category': 'quality_gates',
                'automation_strategy': 'automatic_retry',
                'failure_detected': True
            }
        
        def handle_dependency_recovery(failure_category):
            \"\"\"Handle dependency-related failures with automated updates\"\"\"
            print('[PACKAGE] DEPENDENCY RECOVERY HANDLER')
            
            recovery_actions = []
            
            # Python dependency recovery
            if Path('requirements.txt').exists():
                print('  ðŸ Analyzing Python dependencies...')
                
                # Check for known vulnerable packages
                try:
                    result = subprocess.run(['safety', 'check', '--json'], 
                                          capture_output=True, text=True, timeout=60)
                    if result.returncode != 0:
                        print('  [WARN] Security vulnerabilities detected in Python packages')
                        recovery_actions.append({
                            'action': 'python_security_update',
                            'status': 'identified',
                            'details': 'Safety check found vulnerabilities'
                        })
                        
                        # Attempt automatic security updates
                        update_result = attempt_python_security_updates()
                        recovery_actions.extend(update_result)
                        
                except Exception as e:
                    print(f'  Warning: Safety check failed: {e}')
            
            # Node.js dependency recovery
            if Path('package.json').exists():
                print('  [PACKAGE] Analyzing Node.js dependencies...')
                
                try:
                    result = subprocess.run(['npm', 'audit', '--json'], 
                                          capture_output=True, text=True, timeout=60)
                    if result.returncode != 0:
                        print('  [WARN] Security vulnerabilities detected in npm packages')
                        recovery_actions.append({
                            'action': 'npm_security_update',
                            'status': 'identified',
                            'details': 'npm audit found vulnerabilities'
                        })
                        
                        # Attempt automatic security updates
                        update_result = attempt_npm_security_updates()
                        recovery_actions.extend(update_result)
                        
                except Exception as e:
                    print(f'  Warning: npm audit failed: {e}')
            
            return recovery_actions
        
        def handle_quality_gate_recovery(failure_category):
            \"\"\"Handle quality gate failures with automated fixes\"\"\"
            print('[SPARKLE] QUALITY GATE RECOVERY HANDLER')
            
            recovery_actions = []
            
            # Code formatting recovery
            print('  [ART] Applying automated code formatting...')
            
            # Python code formatting
            python_files = list(Path('.').glob('**/*.py'))
            if python_files:
                try:
                    # Apply Black formatting
                    subprocess.run(['black', '.', '--exclude', 'venv|env|__pycache__'], 
                                 check=False, timeout=120)
                    
                    # Apply isort import sorting
                    subprocess.run(['isort', '.', '--skip', 'venv', '--skip', 'env'], 
                                 check=False, timeout=60)
                    
                    recovery_actions.append({
                        'action': 'python_formatting',
                        'status': 'applied',
                        'files_processed': len(python_files)
                    })
                    
                except Exception as e:
                    print(f'  Warning: Python formatting failed: {e}')
            
            # Remove unused imports and variables
            if python_files:
                try:
                    subprocess.run(['autoflake', '--remove-all-unused-imports', 
                                  '--remove-unused-variables', '--in-place', '--recursive', '.'], 
                                 check=False, timeout=120)
                    
                    recovery_actions.append({
                        'action': 'unused_code_removal',
                        'status': 'applied',
                        'details': 'Removed unused imports and variables'
                    })
                    
                except Exception as e:
                    print(f'  Warning: Unused code removal failed: {e}')
            
            return recovery_actions
        
        def handle_security_recovery(failure_category):
            \"\"\"Handle security-related failures\"\"\"
            print('[SHIELD] SECURITY RECOVERY HANDLER')
            
            recovery_actions = []
            
            # Run comprehensive security scan
            print('  [SEARCH] Running security analysis...')
            
            try:
                # Bandit security scan for Python
                result = subprocess.run(['bandit', '-r', '.', '-f', 'json', '-o', '/tmp/bandit-results.json'], 
                                      capture_output=True, text=True, timeout=300)
                
                if Path('/tmp/bandit-results.json').exists():
                    with open('/tmp/bandit-results.json', 'r') as f:
                        bandit_data = json.load(f)
                        
                    issues_found = len(bandit_data.get('results', []))
                    
                    recovery_actions.append({
                        'action': 'security_scan',
                        'status': 'completed',
                        'issues_found': issues_found,
                        'tool': 'bandit'
                    })
                    
            except Exception as e:
                print(f'  Warning: Bandit scan failed: {e}')
            
            return recovery_actions
        
        def handle_performance_recovery(failure_category):
            \"\"\"Handle performance-related failures\"\"\"
            print('[LIGHTNING] PERFORMANCE RECOVERY HANDLER')
            
            recovery_actions = []
            
            # Analyze and optimize Python code performance
            print('  [CHART] Analyzing performance bottlenecks...')
            
            # Look for common performance issues
            performance_issues = scan_performance_issues()
            
            recovery_actions.append({
                'action': 'performance_analysis',
                'status': 'completed',
                'issues_identified': len(performance_issues),
                'recommendations': performance_issues[:5]  # Top 5 recommendations
            })
            
            return recovery_actions
        
        def attempt_python_security_updates():
            \"\"\"Attempt to update vulnerable Python packages\"\"\"
            try:
                # Get outdated packages
                result = subprocess.run(['pip', 'list', '--outdated', '--format=json'], 
                                      capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0:
                    outdated_packages = json.loads(result.stdout)
                    
                    security_updates = []
                    for package in outdated_packages[:10]:  # Limit to 10 packages
                        package_name = package['name']
                        current_version = package['version']
                        latest_version = package['latest_version']
                        
                        print(f'    [PACKAGE] {package_name}: {current_version} -> {latest_version}')
                        security_updates.append({
                            'package': package_name,
                            'current': current_version,
                            'latest': latest_version
                        })
                    
                    return [{
                        'action': 'python_package_analysis',
                        'status': 'completed',
                        'updates_available': security_updates
                    }]
                    
            except Exception as e:
                print(f'    Warning: Package update analysis failed: {e}')
            
            return []
        
        def attempt_npm_security_updates():
            \"\"\"Attempt to update vulnerable npm packages\"\"\"
            try:
                # Run npm audit fix in dry-run mode
                result = subprocess.run(['npm', 'audit', 'fix', '--dry-run', '--json'], 
                                      capture_output=True, text=True, timeout=120)
                
                return [{
                    'action': 'npm_audit_fix_analysis',
                    'status': 'completed',
                    'details': 'Analyzed potential npm fixes'
                }]
                
            except Exception as e:
                print(f'    Warning: npm audit fix analysis failed: {e}')
                
            return []
        
        def scan_performance_issues():
            \"\"\"Scan for common performance issues\"\"\"
            issues = []
            
            # Check for large files that might impact performance
            large_files = []
            for py_file in Path('.').glob('**/*.py'):
                try:
                    if py_file.stat().st_size > 50000:  # Files larger than 50KB
                        large_files.append(str(py_file))
                except:
                    continue
            
            if large_files:
                issues.append(f'Large Python files detected ({len(large_files)} files > 50KB)')
            
            # Check for potential inefficient patterns
            if Path('requirements.txt').exists():
                with open('requirements.txt', 'r') as f:
                    deps = f.read()
                    if 'pandas' in deps and 'numpy' not in deps:
                        issues.append('pandas used without numpy - consider explicit numpy dependency')
            
            return issues
        
        def save_recovery_results(stream_name, results):
            \"\"\"Save recovery results for later consolidation\"\"\"
            recovery_report = {
                'timestamp': datetime.now().isoformat(),
                'recovery_stream': stream_name,
                'actions_performed': len(results),
                'results': results,
                'success_rate': calculate_success_rate(results)
            }
            
            Path('.github/automation/recovery').mkdir(parents=True, exist_ok=True)
            with open(f'.github/automation/recovery/{stream_name}_results.json', 'w') as f:
                json.dump(recovery_report, f, indent=2)
            
            print(f'\\n[OK] Recovery stream completed: {len(results)} actions performed')
        
        def calculate_success_rate(results):
            \"\"\"Calculate success rate of recovery actions\"\"\"
            if not results:
                return 0.0
            
            successful_actions = len([r for r in results if r.get('status') in ['completed', 'applied']])
            return (successful_actions / len(results)) * 100
        
        # Execute recovery stream
        execute_recovery_stream()
        "
        
        echo "::endgroup::"

    - name: Upload Recovery Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: recovery-results-${{ matrix.recovery_stream.name }}-${{ github.run_number }}
        path: |
          .github/automation/recovery/
        retention-days: 7

  # Real-time Notification and Status Updates
  realtime_notification_hub:
    needs: [event_detection_hub, intelligent_recovery_orchestrator]
    if: always()
    runs-on: ubuntu-latest
    name: "Real-Time Notification & Status Update Hub"
    timeout-minutes: 15

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        token: ${{ env.GITHUB_TOKEN }}

    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Notification Dependencies
      run: |
        pip install --upgrade pip
        pip install PyGithub requests python-dateutil

    - name: Download All Automation Artifacts
      uses: actions/download-artifact@v4
      with:
        path: ./automation-artifacts
        merge-multiple: true

    - name: Consolidate Results and Send Notifications
      run: |
        echo "::group::Real-Time Notification Processing"
        echo "Consolidating automation results and sending intelligent notifications..."
        
        python -c "
        import os
        import sys
        import json
        import requests
        from datetime import datetime
        from pathlib import Path
        
        def process_notifications():
            print('ðŸ“¢ REAL-TIME NOTIFICATION HUB')
            print('=' * 40)
            
            # Collect all automation results
            automation_summary = collect_automation_results()
            
            # Generate intelligent notifications
            notifications = generate_intelligent_notifications(automation_summary)
            
            # Send notifications via multiple channels
            send_github_notifications(notifications, automation_summary)
            
            return automation_summary
        
        def collect_automation_results():
            \"\"\"Collect results from all automation streams\"\"\"
            print('[CHART] Collecting automation results...')
            
            summary = {
                'timestamp': datetime.now().isoformat(),
                'failure_detection': None,
                'recovery_results': [],
                'overall_status': 'unknown',
                'actions_taken': 0,
                'success_rate': 0.0
            }
            
            # Load failure detection results
            failure_files = list(Path('./automation-artifacts').glob('**/failure_detection.json'))
            if failure_files:
                try:
                    with open(failure_files[0], 'r') as f:
                        summary['failure_detection'] = json.load(f)
                    print(f'  [OK] Loaded failure detection data')
                except Exception as e:
                    print(f'  [WARN] Failed to load failure detection: {e}')
            
            # Load recovery results
            recovery_files = list(Path('./automation-artifacts').glob('**/*_results.json'))
            total_actions = 0
            successful_actions = 0
            
            for recovery_file in recovery_files:
                try:
                    with open(recovery_file, 'r') as f:
                        recovery_data = json.load(f)
                        summary['recovery_results'].append(recovery_data)
                        
                        actions = recovery_data.get('actions_performed', 0)
                        total_actions += actions
                        
                        # Calculate successful actions based on success rate
                        success_rate = recovery_data.get('success_rate', 0.0) / 100
                        successful_actions += int(actions * success_rate)
                        
                    print(f'  [OK] Loaded recovery results: {recovery_file.name}')
                except Exception as e:
                    print(f'  [WARN] Failed to load recovery results: {e}')
            
            summary['actions_taken'] = total_actions
            summary['success_rate'] = (successful_actions / max(total_actions, 1)) * 100
            
            # Determine overall status
            if summary['failure_detection']:
                if summary['failure_detection'].get('recovery_required', False):
                    if summary['success_rate'] >= 75:
                        summary['overall_status'] = 'recovered'
                    elif summary['success_rate'] >= 50:
                        summary['overall_status'] = 'partially_recovered'
                    else:
                        summary['overall_status'] = 'recovery_failed'
                else:
                    summary['overall_status'] = 'monitoring'
            else:
                summary['overall_status'] = 'healthy'
            
            print(f'\\n[TREND] AUTOMATION SUMMARY:')
            print(f'  - Overall status: {summary[\"overall_status\"]}')
            print(f'  - Actions taken: {summary[\"actions_taken\"]}')
            print(f'  - Success rate: {summary[\"success_rate\"]:.1f}%')
            
            return summary
        
        def generate_intelligent_notifications(summary):
            \"\"\"Generate context-aware notifications\"\"\"
            print('[BRAIN] Generating intelligent notifications...')
            
            notifications = []
            
            status = summary['overall_status']
            actions_taken = summary['actions_taken']
            success_rate = summary['success_rate']
            
            # Status-based notifications
            if status == 'recovered':
                notifications.append({
                    'type': 'success',
                    'title': '[OK] Automated Recovery Successful',
                    'message': f'CI/CD issues automatically resolved with {actions_taken} recovery actions (success rate: {success_rate:.1f}%)',
                    'priority': 'medium'
                })
            elif status == 'partially_recovered':
                notifications.append({
                    'type': 'warning',
                    'title': '[WARN] Partial Recovery Completed',
                    'message': f'Some CI/CD issues resolved automatically. {actions_taken} actions taken with {success_rate:.1f}% success rate. Manual review recommended.',
                    'priority': 'high'
                })
            elif status == 'recovery_failed':
                notifications.append({
                    'type': 'error',
                    'title': '[FAIL] Automated Recovery Failed',
                    'message': f'Attempted {actions_taken} recovery actions but issues persist (success rate: {success_rate:.1f}%). Manual intervention required.',
                    'priority': 'critical'
                })
            elif status == 'monitoring':
                notifications.append({
                    'type': 'info',
                    'title': 'ðŸ‘ï¸ CI/CD Monitoring Active',
                    'message': 'No critical issues detected. Continuous monitoring active.',
                    'priority': 'low'
                })
            
            # Recovery-specific notifications
            recovery_results = summary.get('recovery_results', [])
            if recovery_results:
                for recovery in recovery_results:
                    stream_name = recovery.get('recovery_stream', 'unknown')
                    stream_actions = recovery.get('actions_performed', 0)
                    
                    if stream_actions > 0:
                        notifications.append({
                            'type': 'info',
                            'title': f'[WRENCH] {stream_name.replace(\"_\", \" \").title()} Recovery',
                            'message': f'Performed {stream_actions} automated recovery actions',
                            'priority': 'low'
                        })
            
            return notifications
        
        def send_github_notifications(notifications, summary):
            \"\"\"Send notifications via GitHub mechanisms\"\"\"
            print('ðŸ“¨ Sending GitHub notifications...')
            
            # Create GitHub issue comment for critical notifications
            critical_notifications = [n for n in notifications if n['priority'] in ['critical', 'high']]
            
            if critical_notifications and os.environ.get('GITHUB_TOKEN'):
                try:
                    create_github_issue_notification(critical_notifications, summary)
                except Exception as e:
                    print(f'  [WARN] Failed to create GitHub notification: {e}')
            
            # Update commit status
            try:
                update_commit_status(summary)
            except Exception as e:
                print(f'  [WARN] Failed to update commit status: {e}')
            
            # Generate workflow summary
            generate_workflow_summary(notifications, summary)
        
        def create_github_issue_notification(critical_notifications, summary):
            \"\"\"Create GitHub issue or comment for critical notifications\"\"\"
            github_token = os.environ.get('GITHUB_TOKEN')
            repo_name = os.environ.get('GITHUB_REPOSITORY')
            
            if not all([github_token, repo_name]):
                print('  [WARN] GitHub API credentials unavailable for issue creation')
                return
            
            headers = {
                'Authorization': f'token {github_token}',
                'Accept': 'application/vnd.github.v3+json'
            }
            
            # Create issue body
            issue_body = generate_issue_body(critical_notifications, summary)
            
            # Check if we're in a PR context
            if os.environ.get('GITHUB_EVENT_NAME') == 'pull_request':
                # Add PR comment
                pr_number = os.environ.get('GITHUB_REF', '').split('/')[-2]
                if pr_number.isdigit():
                    comment_url = f'https://api.github.com/repos/{repo_name}/issues/{pr_number}/comments'
                    comment_data = {'body': issue_body}
                    
                    response = requests.post(comment_url, headers=headers, json=comment_data)
                    if response.status_code == 201:
                        print(f'  [OK] Created PR comment for notifications')
                    else:
                        print(f'  [WARN] Failed to create PR comment: {response.status_code}')
        
        def update_commit_status(summary):
            \"\"\"Update commit status based on automation results\"\"\"
            github_token = os.environ.get('GITHUB_TOKEN')
            repo_name = os.environ.get('GITHUB_REPOSITORY')
            commit_sha = os.environ.get('GITHUB_SHA')
            
            if not all([github_token, repo_name, commit_sha]):
                print('  [WARN] GitHub API credentials unavailable for status update')
                return
            
            headers = {
                'Authorization': f'token {github_token}',
                'Accept': 'application/vnd.github.v3+json'
            }
            
            status_url = f'https://api.github.com/repos/{repo_name}/statuses/{commit_sha}'
            
            # Determine status based on automation results
            status_mapping = {
                'healthy': 'success',
                'monitoring': 'success',
                'recovered': 'success',
                'partially_recovered': 'pending',
                'recovery_failed': 'failure'
            }
            
            github_status = status_mapping.get(summary['overall_status'], 'pending')
            
            status_data = {
                'state': github_status,
                'target_url': f'https://github.com/{repo_name}/actions/runs/{os.environ.get(\"GITHUB_RUN_ID\", \"\")}',
                'description': f'Closed-loop automation: {summary[\"overall_status\"]} ({summary[\"actions_taken\"]} actions)',
                'context': 'continuous-integration/closed-loop-automation'
            }
            
            response = requests.post(status_url, headers=headers, json=status_data)
            if response.status_code == 201:
                print(f'  [OK] Updated commit status: {github_status}')
            else:
                print(f'  [WARN] Failed to update commit status: {response.status_code}')
        
        def generate_issue_body(notifications, summary):
            \"\"\"Generate formatted issue body for GitHub notifications\"\"\"
            body_parts = [
                '## ðŸ¤– Closed-Loop Automation Report',
                '',
                f'**Status**: {summary[\"overall_status\"].replace(\"_\", \" \").title()}',
                f'**Actions Taken**: {summary[\"actions_taken\"]}',
                f'**Success Rate**: {summary[\"success_rate\"]:.1f}%',
                f'**Timestamp**: {summary[\"timestamp\"]}',
                ''
            ]
            
            if notifications:
                body_parts.extend([
                    '### ðŸ“¢ Critical Notifications:',
                    ''
                ])
                
                for notification in notifications:
                    body_parts.append(f'**{notification[\"title\"]}**')
                    body_parts.append(notification['message'])
                    body_parts.append('')
            
            # Add recovery details
            recovery_results = summary.get('recovery_results', [])
            if recovery_results:
                body_parts.extend([
                    '### [WRENCH] Recovery Actions:',
                    ''
                ])
                
                for recovery in recovery_results:
                    stream_name = recovery.get('recovery_stream', 'unknown').replace('_', ' ').title()
                    actions = recovery.get('actions_performed', 0)
                    success_rate = recovery.get('success_rate', 0.0)
                    
                    body_parts.append(f'- **{stream_name}**: {actions} actions ({success_rate:.1f}% success)')
            
            body_parts.extend([
                '',
                '---',
                '*This notification was generated automatically by the Closed-Loop GitHub Automation system.*'
            ])
            
            return '\\n'.join(body_parts)
        
        def generate_workflow_summary(notifications, summary):
            \"\"\"Generate workflow run summary\"\"\"
            print('ðŸ“ Generating workflow summary...')
            
            summary_lines = [
                '# ðŸ¤– Closed-Loop Automation Summary',
                '',
                f'**Overall Status**: {summary[\"overall_status\"].replace(\"_\", \" \").title()}',
                f'**Actions Performed**: {summary[\"actions_taken\"]}',
                f'**Success Rate**: {summary[\"success_rate\"]:.1f}%',
                ''
            ]
            
            if notifications:
                summary_lines.extend([
                    '## ðŸ“¢ Key Notifications:',
                    ''
                ])
                
                for notification in notifications[:3]:  # Top 3 notifications
                    summary_lines.append(f'- {notification[\"title\"]}: {notification[\"message\"]}')
            
            # Write to GitHub step summary
            github_step_summary = os.environ.get('GITHUB_STEP_SUMMARY')
            if github_step_summary:
                try:
                    with open(github_step_summary, 'w') as f:
                        f.write('\\n'.join(summary_lines))
                    print('  [OK] Generated GitHub step summary')
                except Exception as e:
                    print(f'  [WARN] Failed to write step summary: {e}')
        
        # Execute notification processing
        process_notifications()
        "
        
        echo "::endgroup::"

    - name: Upload Notification Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: notification-results-${{ github.run_number }}
        path: |
          .github/automation/
        retention-days: 30

  # Quality Gate Integration with Real-time Feedback
  quality_gate_integration:
    needs: [event_detection_hub, intelligent_recovery_orchestrator]
    if: always()
    runs-on: ubuntu-latest
    name: "Quality Gate Integration & Status Checks"
    timeout-minutes: 20

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        token: ${{ env.GITHUB_TOKEN }}

    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Quality Gate Dependencies
      run: |
        pip install --upgrade pip
        pip install PyGithub requests pyyaml

    - name: Download Automation Results
      uses: actions/download-artifact@v4
      with:
        path: ./automation-results
        merge-multiple: true

    - name: Update Quality Gate Status Checks
      run: |
        echo "::group::Quality Gate Status Check Updates"
        echo "Updating GitHub status checks based on automation results..."
        
        python -c "
        import os
        import sys
        import json
        import requests
        from datetime import datetime
        from pathlib import Path
        
        def update_quality_gate_checks():
            print('[TARGET] QUALITY GATE INTEGRATION SYSTEM')
            print('=' * 45)
            
            # Load automation results
            automation_data = load_automation_data()
            
            # Generate quality gate status
            quality_status = generate_quality_gate_status(automation_data)
            
            # Update GitHub status checks
            update_github_checks(quality_status)
            
            return quality_status
        
        def load_automation_data():
            \"\"\"Load all automation results for quality assessment\"\"\"
            print('[CHART] Loading automation data for quality assessment...')
            
            data = {
                'failure_detection': None,
                'recovery_results': [],
                'overall_metrics': {
                    'failures_detected': 0,
                    'actions_taken': 0,
                    'success_rate': 0.0,
                    'recovery_time_minutes': 0
                }
            }
            
            # Load failure detection
            failure_files = list(Path('./automation-results').glob('**/failure_detection.json'))
            if failure_files:
                with open(failure_files[0], 'r') as f:
                    data['failure_detection'] = json.load(f)
                    data['overall_metrics']['failures_detected'] = 1 if data['failure_detection'].get('failure_detected') else 0
            
            # Load recovery results
            recovery_files = list(Path('./automation-results').glob('**/*_results.json'))
            total_actions = 0
            total_success = 0
            
            for recovery_file in recovery_files:
                with open(recovery_file, 'r') as f:
                    recovery_data = json.load(f)
                    data['recovery_results'].append(recovery_data)
                    
                    actions = recovery_data.get('actions_performed', 0)
                    success_rate = recovery_data.get('success_rate', 0.0)
                    
                    total_actions += actions
                    total_success += (actions * success_rate / 100)
            
            data['overall_metrics']['actions_taken'] = total_actions
            data['overall_metrics']['success_rate'] = (total_success / max(total_actions, 1)) * 100 if total_actions > 0 else 100.0
            
            return data
        
        def generate_quality_gate_status(automation_data):
            \"\"\"Generate comprehensive quality gate status\"\"\"
            print('[TARGET] Generating quality gate status...')
            
            metrics = automation_data['overall_metrics']
            failure_data = automation_data.get('failure_detection', {})
            
            # Quality gate thresholds
            thresholds = {
                'max_failures_detected': 3,
                'min_success_rate': 75.0,
                'max_recovery_time': 60
            }
            
            # Evaluate quality gates
            gates = {
                'failure_detection': {
                    'status': 'success' if metrics['failures_detected'] <= thresholds['max_failures_detected'] else 'failure',
                    'description': f'Failures detected: {metrics[\"failures_detected\"]}/{thresholds[\"max_failures_detected\"]}',
                    'context': 'quality-gate/failure-detection'
                },
                'recovery_effectiveness': {
                    'status': 'success' if metrics['success_rate'] >= thresholds['min_success_rate'] else 'failure',
                    'description': f'Recovery success rate: {metrics[\"success_rate\"]:.1f}% (min: {thresholds[\"min_success_rate\"]}%)',
                    'context': 'quality-gate/recovery-effectiveness'
                },
                'automation_health': {
                    'status': 'success' if metrics['actions_taken'] > 0 or metrics['failures_detected'] == 0 else 'pending',
                    'description': f'Automation actions: {metrics[\"actions_taken\"]} performed',
                    'context': 'quality-gate/automation-health'
                }
            }
            
            # Overall quality gate status
            individual_statuses = [gate['status'] for gate in gates.values()]
            
            if all(status == 'success' for status in individual_statuses):
                overall_status = 'success'
            elif any(status == 'failure' for status in individual_statuses):
                overall_status = 'failure'
            else:
                overall_status = 'pending'
            
            gates['overall'] = {
                'status': overall_status,
                'description': f'Overall automation quality: {overall_status}',
                'context': 'quality-gate/closed-loop-automation'
            }
            
            print(f'  [CHART] Quality gate status: {overall_status}')
            for gate_name, gate_info in gates.items():
                print(f'    - {gate_name}: {gate_info[\"status\"]} - {gate_info[\"description\"]}')
            
            return gates
        
        def update_github_checks(quality_gates):
            \"\"\"Update GitHub status checks for quality gates\"\"\"
            print('ðŸ“ Updating GitHub status checks...')
            
            github_token = os.environ.get('GITHUB_TOKEN')
            repo_name = os.environ.get('GITHUB_REPOSITORY')
            commit_sha = os.environ.get('GITHUB_SHA')
            
            if not all([github_token, repo_name, commit_sha]):
                print('  [WARN] GitHub API credentials unavailable - skipping status updates')
                return
            
            headers = {
                'Authorization': f'token {github_token}',
                'Accept': 'application/vnd.github.v3+json'
            }
            
            base_url = f'https://api.github.com/repos/{repo_name}/statuses/{commit_sha}'
            
            for gate_name, gate_info in quality_gates.items():
                status_data = {
                    'state': gate_info['status'],
                    'target_url': f'https://github.com/{repo_name}/actions/runs/{os.environ.get(\"GITHUB_RUN_ID\", \"\")}',
                    'description': gate_info['description'],
                    'context': gate_info['context']
                }
                
                try:
                    response = requests.post(base_url, headers=headers, json=status_data)
                    if response.status_code == 201:
                        print(f'  [OK] Updated {gate_name} status check')
                    else:
                        print(f'  [WARN] Failed to update {gate_name} status: {response.status_code}')
                except Exception as e:
                    print(f'  [WARN] Error updating {gate_name} status: {e}')
            
            print('[OK] Quality gate status updates completed')
        
        # Execute quality gate integration
        update_quality_gate_checks()
        "
        
        echo "::endgroup::"

  # Comprehensive Automation Report
  automation_report_generator:
    needs: [event_detection_hub, intelligent_recovery_orchestrator, realtime_notification_hub, quality_gate_integration]
    if: always()
    runs-on: ubuntu-latest
    name: "Comprehensive Automation Report Generator"
    timeout-minutes: 15

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4

    - name: Set up Python Environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Download All Automation Artifacts
      uses: actions/download-artifact@v4
      with:
        path: ./complete-automation-results
        merge-multiple: true

    - name: Generate Comprehensive Report
      run: |
        echo "::group::Comprehensive Automation Report Generation"
        echo "Generating final comprehensive report of all closed-loop automation activities..."
        
        python -c "
        import os
        import json
        from datetime import datetime
        from pathlib import Path
        
        def generate_comprehensive_report():
            print('[CLIPBOARD] COMPREHENSIVE AUTOMATION REPORT GENERATOR')
            print('=' * 50)
            
            # Collect all automation data
            report_data = collect_comprehensive_data()
            
            # Generate detailed report
            comprehensive_report = create_comprehensive_report(report_data)
            
            # Save reports in multiple formats
            save_comprehensive_reports(comprehensive_report)
            
            return comprehensive_report
        
        def collect_comprehensive_data():
            \"\"\"Collect all automation data from artifacts\"\"\"
            print('[CHART] Collecting comprehensive automation data...')
            
            data = {
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'workflow_run_id': os.environ.get('GITHUB_RUN_ID', 'unknown'),
                    'commit_sha': os.environ.get('GITHUB_SHA', 'unknown'),
                    'repository': os.environ.get('GITHUB_REPOSITORY', 'unknown'),
                    'automation_version': os.environ.get('AUTOMATION_VERSION', 'unknown')
                },
                'failure_detection': None,
                'recovery_streams': [],
                'notifications': [],
                'quality_gates': {}
            }
            
            # Load failure detection
            failure_files = list(Path('./complete-automation-results').glob('**/failure_detection.json'))
            if failure_files:
                with open(failure_files[0], 'r') as f:
                    data['failure_detection'] = json.load(f)
                print('  [OK] Loaded failure detection data')
            
            # Load recovery results
            recovery_files = list(Path('./complete-automation-results').glob('**/*_results.json'))
            for recovery_file in recovery_files:
                if 'failure_detection' not in recovery_file.name:
                    with open(recovery_file, 'r') as f:
                        data['recovery_streams'].append(json.load(f))
            print(f'  [OK] Loaded {len(data[\"recovery_streams\"])} recovery streams')
            
            return data
        
        def create_comprehensive_report(data):
            \"\"\"Create detailed comprehensive report\"\"\"
            print('ðŸ“ Creating comprehensive automation report...')
            
            # Calculate summary metrics
            total_actions = sum(stream.get('actions_performed', 0) for stream in data['recovery_streams'])
            
            avg_success_rate = 0.0
            if data['recovery_streams']:
                success_rates = [stream.get('success_rate', 0.0) for stream in data['recovery_streams']]
                avg_success_rate = sum(success_rates) / len(success_rates)
            
            failure_detected = data.get('failure_detection', {}).get('failure_detected', False)
            failure_category = data.get('failure_detection', {}).get('failure_category', 'none')
            
            # Determine overall automation effectiveness
            if not failure_detected:
                effectiveness = 'monitoring'
            elif total_actions == 0:
                effectiveness = 'detection_only'
            elif avg_success_rate >= 80:
                effectiveness = 'highly_effective'
            elif avg_success_rate >= 60:
                effectiveness = 'moderately_effective'
            else:
                effectiveness = 'low_effectiveness'
            
            report = {
                'executive_summary': {
                    'automation_effectiveness': effectiveness,
                    'total_recovery_actions': total_actions,
                    'average_success_rate': round(avg_success_rate, 2),
                    'primary_failure_category': failure_category,
                    'automation_triggered': failure_detected
                },
                'detailed_analysis': {
                    'failure_detection': data.get('failure_detection'),
                    'recovery_streams': data['recovery_streams'],
                    'stream_performance': analyze_stream_performance(data['recovery_streams'])
                },
                'recommendations': generate_automation_recommendations(data, effectiveness, avg_success_rate),
                'metadata': data['metadata'],
                'report_version': '2.0.0'
            }
            
            return report
        
        def analyze_stream_performance(recovery_streams):
            \"\"\"Analyze performance of individual recovery streams\"\"\"
            performance_analysis = {}
            
            for stream in recovery_streams:
                stream_name = stream.get('recovery_stream', 'unknown')
                
                performance_analysis[stream_name] = {
                    'actions_performed': stream.get('actions_performed', 0),
                    'success_rate': stream.get('success_rate', 0.0),
                    'execution_time': stream.get('execution_time_seconds', 0),
                    'effectiveness_rating': rate_stream_effectiveness(stream)
                }
            
            return performance_analysis
        
        def rate_stream_effectiveness(stream):
            \"\"\"Rate the effectiveness of a recovery stream\"\"\"
            actions = stream.get('actions_performed', 0)
            success_rate = stream.get('success_rate', 0.0)
            
            if actions == 0:
                return 'no_action_needed'
            elif success_rate >= 90:
                return 'excellent'
            elif success_rate >= 70:
                return 'good'
            elif success_rate >= 50:
                return 'fair'
            else:
                return 'poor'
        
        def generate_automation_recommendations(data, effectiveness, avg_success_rate):
            \"\"\"Generate actionable recommendations for automation improvement\"\"\"
            recommendations = []
            
            # Effectiveness-based recommendations
            if effectiveness == 'low_effectiveness':
                recommendations.append({
                    'category': 'improvement',
                    'priority': 'high',
                    'recommendation': 'Review and enhance recovery algorithms - success rate below 60%'
                })
            
            if effectiveness == 'detection_only':
                recommendations.append({
                    'category': 'enhancement',
                    'priority': 'medium',
                    'recommendation': 'Implement additional recovery actions for detected failure categories'
                })
            
            # Stream-specific recommendations
            recovery_streams = data.get('recovery_streams', [])
            if not recovery_streams:
                recommendations.append({
                    'category': 'configuration',
                    'priority': 'high',
                    'recommendation': 'No recovery streams executed - verify automation configuration'
                })
            
            # Success rate recommendations
            if avg_success_rate < 75:
                recommendations.append({
                    'category': 'optimization',
                    'priority': 'medium',
                    'recommendation': 'Optimize recovery success rate through better failure categorization'
                })
            
            # Best practices recommendations
            recommendations.extend([
                {
                    'category': 'monitoring',
                    'priority': 'low',
                    'recommendation': 'Continue monitoring automation effectiveness and adjust thresholds as needed'
                },
                {
                    'category': 'documentation',
                    'priority': 'low', 
                    'recommendation': 'Document successful recovery patterns for future automation enhancement'
                }
            ])
            
            return recommendations
        
        def save_comprehensive_reports(report):
            \"\"\"Save comprehensive report in multiple formats\"\"\"
            print('[DISK] Saving comprehensive reports...')
            
            # Create reports directory
            Path('.github/automation/reports').mkdir(parents=True, exist_ok=True)
            
            # Save JSON report
            json_filename = f'automation_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'
            with open(f'.github/automation/reports/{json_filename}', 'w') as f:
                json.dump(report, f, indent=2, default=str)
            print(f'  [OK] Saved JSON report: {json_filename}')
            
            # Generate Markdown summary
            markdown_report = generate_markdown_summary(report)
            markdown_filename = f'automation_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.md'
            with open(f'.github/automation/reports/{markdown_filename}', 'w') as f:
                f.write(markdown_report)
            print(f'  [OK] Saved Markdown summary: {markdown_filename}')
            
            # Generate GitHub step summary
            github_summary = os.environ.get('GITHUB_STEP_SUMMARY')
            if github_summary:
                with open(github_summary, 'w') as f:
                    f.write(markdown_report)
                print('  [OK] Updated GitHub step summary')
        
        def generate_markdown_summary(report):
            \"\"\"Generate formatted Markdown summary\"\"\"
            exec_summary = report['executive_summary']
            
            summary_lines = [
                '# ðŸ¤– Closed-Loop GitHub Automation Report',
                '',
                f'**Generated**: {report[\"metadata\"][\"generated_at\"]}',
                f'**Repository**: {report[\"metadata\"][\"repository\"]}',
                f'**Workflow Run**: {report[\"metadata\"][\"workflow_run_id\"]}',
                '',
                '## [CHART] Executive Summary',
                '',
                f'- **Automation Effectiveness**: {exec_summary[\"automation_effectiveness\"].replace(\"_\", \" \").title()}',
                f'- **Total Recovery Actions**: {exec_summary[\"total_recovery_actions\"]}',
                f'- **Average Success Rate**: {exec_summary[\"average_success_rate\"]}%',
                f'- **Primary Failure Category**: {exec_summary[\"primary_failure_category\"].replace(\"_\", \" \").title()}',
                f'- **Automation Triggered**: {\"Yes\" if exec_summary[\"automation_triggered\"] else \"No\"}',
                ''
            ]
            
            # Add recovery stream details
            detailed_analysis = report.get('detailed_analysis', {})
            stream_performance = detailed_analysis.get('stream_performance', {})
            
            if stream_performance:
                summary_lines.extend([
                    '## [WRENCH] Recovery Stream Performance',
                    ''
                ])
                
                for stream_name, performance in stream_performance.items():
                    stream_title = stream_name.replace('_', ' ').title()
                    effectiveness = performance['effectiveness_rating'].replace('_', ' ').title()
                    
                    summary_lines.extend([
                        f'### {stream_title}',
                        f'- Actions Performed: {performance[\"actions_performed\"]}',
                        f'- Success Rate: {performance[\"success_rate\"]}%',
                        f'- Effectiveness: {effectiveness}',
                        ''
                    ])
            
            # Add recommendations
            recommendations = report.get('recommendations', [])
            if recommendations:
                summary_lines.extend([
                    '## [BULB] Recommendations',
                    ''
                ])
                
                high_priority = [r for r in recommendations if r['priority'] == 'high']
                medium_priority = [r for r in recommendations if r['priority'] == 'medium']
                
                if high_priority:
                    summary_lines.extend(['### High Priority', ''])
                    for rec in high_priority:
                        summary_lines.append(f'- {rec[\"recommendation\"]}')
                    summary_lines.append('')
                
                if medium_priority:
                    summary_lines.extend(['### Medium Priority', ''])
                    for rec in medium_priority:
                        summary_lines.append(f'- {rec[\"recommendation\"]}')
            
            summary_lines.extend([
                '',
                '---',
                '*This report was generated automatically by the Closed-Loop GitHub Automation system.*'
            ])
            
            return '\\n'.join(summary_lines)
        
        # Generate comprehensive report
        generate_comprehensive_report()
        "
        
        echo "::endgroup::"

    - name: Upload Final Automation Report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: comprehensive-automation-report-${{ github.run_number }}
        path: |
          .github/automation/reports/
        retention-days: 90