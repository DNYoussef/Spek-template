name: Performance Monitoring
on:
  push:
    branches: [main]
    paths:
      - 'analyzer/**'
      - 'src/**'
      - '**/*.py'
  pull_request:
    branches: [main]
    paths:
      - 'analyzer/**'
      - 'src/**'
      - '**/*.py'
  workflow_dispatch:
    inputs:
      trigger-reason:
        description: 'Reason for triggering analysis'
        required: false
        default: 'manual'

jobs:
  performance-monitoring:
    runs-on: ubuntu-latest
    name: "Performance Monitoring & Optimization"
    timeout-minutes: 45
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        if [ -f setup.py ]; then
          pip install -e .
        fi

    - name: Create Artifacts Directory
      run: mkdir -p .claude/.artifacts

    - name: SUCCESS Performance Monitoring and Optimization
      run: |
        echo "SUCCESS: Running performance monitoring and cache optimization..."
        export PYTHONPATH="${GITHUB_WORKSPACE}/analyzer:${PYTHONPATH}"
        python .github/scripts/run_performance_monitor.py

    - name: CHART Performance Analysis
      run: |
        if [ -f .claude/.artifacts/performance_monitor.json ]; then
          echo "=== Performance Monitoring Summary ==="
          python -c "exec('''import json; exec(\"\"\"with open(\\\".claude/.artifacts/performance_monitor.json\\\", \\\"r\\\") as f: data = json.load(f)\\nutilization = data.get(\\\"resource_utilization\\\", {})\\ncpu = utilization.get(\\\"cpu_usage\\\", {})\\nmemory = utilization.get(\\\"memory_usage\\\", {})\\nrecommendations = data.get(\\\"optimization_recommendations\\\", [])\\nprint(f\\\"CPU Efficiency Score: {cpu.get(\\\\\\\"efficiency_score\\\\\\\", \\\\\\\"N/A\\\\\\\")}\\\")\\nprint(f\\\"Memory Optimization Score: {memory.get(\\\\\\\"optimization_score\\\\\\\", \\\\\\\"N/A\\\\\\\")}\\\")\\nprint(f\\\"Optimization Recommendations: {len(recommendations)}\\\")\\nif data.get(\\\"fallback\\\"): print(\\\"WARNING: Analysis ran in fallback mode\\\")\\nfor i, rec in enumerate(recommendations[:3], 1): print(f\\\"{i}. {rec}\\\")\\n\\\"\"\")''')"
        else
          echo "ERROR: Performance monitoring file not found"
          exit 1
        fi

    - name: UPLOAD Upload Performance Analysis
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-monitoring-${{ github.run_number }}
        path: |
          .claude/.artifacts/performance_monitor.json

    - name: LIGHTNING Performance Quality Gate
      run: |
        echo "=== Performance Quality Gate ==="
        python -c "exec('''import json; import sys; exec(\"\"\"with open(\\\".claude/.artifacts/performance_monitor.json\\\", \\\"r\\\") as f: data = json.load(f)\\nmin_cpu_efficiency = 0.70\\nmin_memory_optimization = 0.65\\nutilization = data.get(\\\"resource_utilization\\\", {})\\ncpu_score = utilization.get(\\\"cpu_usage\\\", {}).get(\\\"efficiency_score\\\", 0)\\nmemory_score = utilization.get(\\\"memory_usage\\\", {}).get(\\\"optimization_score\\\", 0)\\nfailed = False\\nif cpu_score < min_cpu_efficiency:\\n    print(f\\\"ERROR: CPU efficiency: {cpu_score:.2%} < {min_cpu_efficiency:.2%}\\\")\\n    failed = True\\nelse:\\n    print(f\\\"SUCCESS: CPU efficiency: {cpu_score:.2%} >= {min_cpu_efficiency:.2%}\\\")\\nif memory_score < min_memory_optimization:\\n    print(f\\\"ERROR: Memory optimization: {memory_score:.2%} < {min_memory_optimization:.2%}\\\")\\n    failed = True\\nelse:\\n    print(f\\\"SUCCESS: Memory optimization: {memory_score:.2%} >= {min_memory_optimization:.2%}\\\")\\nif failed:\\n    print(\\\"\\\\nWARNING: Performance quality gate has warnings\\\")\\n    print(\\\"WRENCH: Consider reviewing optimization recommendations\\\")\\nelse:\\n    print(\\\"\\\\nSUCCESS: Performance quality gate PASSED\\\")\\n\\\"\"\")''')"